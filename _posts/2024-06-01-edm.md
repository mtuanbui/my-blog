---
layout: distill
title: Elucidating the design space of diffusion-based generative models (a.k.a EDM model)
description: notes on the outstanding EDM paper
tags: neurips2022 diffusion-model score-based-model
categories: review
giscus_comments: false
date: 2024-10-10
featured: false
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

# authors:
#   - name: Albert Einstein
#     url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#     affiliations:
#       name: IAS, Princeton
#   - name: Boris Podolsky
#     url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#     affiliations:
#       name: IAS, Princeton
#   - name: Nathan Rosen
#     url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#     affiliations:
#       name: IAS, Princeton

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Contributions
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Paper contents
    subsections:
      - name: 1. Expressing diffusion models in a common framework
      - name: 2. Improvements to deterministic sampling
      - name: 3. Stochastic sampling
      - name: 4. Preconditioning and training
  - name: Applications
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
# _styles: >
#   .fake-img {
#     background: #bbb;
#     border: 1px solid rgba(0, 0, 0, 0.1);
#     box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
#     margin-bottom: 12px;
#   }
#   .fake-img p {
#     font-family: monospace;
#     color: white;
#     text-align: left;
#     margin: 12px 0;
#     text-align: center;
#     font-size: 16px;
#   }
---

The EDM paper was introduced by Karras et al. <d-cite key="karras2022"></d-cite>. This excellent papers received Outstanding Papers Reward in NeurIPS 2022 [https://arxiv.org/abs/2206.00364](https://arxiv.org/abs/2206.00364)

## Contributions

As summarized in <d-footnote>https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/</d-footnote>, the main contributions of the paper are:

1. A **common framework** for diffusion models
2. Design choices related to **sampling** (generating images when you already have a trained denoiser)
3. Design choices when **training** that denoiser

## Paper contents

### 1. Expressing diffusion models in a common framework

The Diffusion ODE can be expressed that:

$$
\mathrm{d}x=[f(t)x-\frac{1}{2}g(t)^2\nabla_x\log p_t(x)]\mathrm{d}t
$$

The issue is $f(t), g(t)$ and $p_t(x)$ not being quite useful when we like to design “working” variants of diffusion model because change in a component would lead to changes in the others in order to ensure the model converge to the data in limit. Therefore, the ODE need to be re-written to eliminate implicit dependencies between the components

**a. Replace $p_t(x)$ by $p(x;\sigma)$**

The perturbation kernels of Diffusion SDEs can be generalized as

$$
\begin{aligned} x_t&=s(t)\left( x_0 + \sigma(t)\epsilon\right) \textrm{ where } \epsilon \sim \mathcal{N}(0,\mathbf{I}) \end{aligned}
$$

For example, SMLD SDE in <d-cite key="song2019"></d-cite> has

$$
\begin{aligned}
s(t)&=1 \\
x(t)&=x_0+\sigma(t) \epsilon
\end{aligned}
$$

And for DDPM SDE in <d-cite key=ho2020></d-cite>,

$$
\begin{aligned}
s(t)&=\frac{1}{\sqrt{\sigma(t)^2+1}} \\
x(t)&=\frac{1}{\sqrt{\sigma(t)^2+1}}(x_0+\sigma(t) \epsilon)
\end{aligned}
$$

Equation 15 ~ 20 in <d-cite key=karras2022></d-cite> proves that the marginal distribution $p_t(x)$ can be expressed as:

$$
p_t(x)=\int p_{t}({x}\mid x_0)\mathrm{~}p_{\mathrm{data}}(x_0)\mathrm{~d}{x}=s(t)^{-d}p\left( \frac{x_t}{s(t)};\sigma(t)\right)
$$

where $d$ is data dimension and $p(x;\sigma)$ is a distribution obtained by adding $\sigma$-standard deviation Gaussian noise to the original signal. Substitute $p_t(x)$ into the ODE equation, we get

\begin{equation}
\label{eq:ode}
\mathrm{d}x = [f(t)x-\frac{1}{2}g(t)^2\nabla_x\log p_t(x)]\mathrm{d}t = \left[f(t){x}-\frac12g(t)^2 \nabla_x\log p(\frac{x}{s(t)};\sigma(t)) \right]\mathrm{~d}t
\end{equation}

(Refer to equation 21~24 in <d-cite key=karras2022></d-cite> for the proof)

$p(x;\sigma)$ is more favorable than $p_t(x)$ because it does not depend on $t$ and thus, the score $\nabla_x\log p(x;\sigma)$ does not depend on $\sigma(t),s(t)$ and how $t$ is discritized

The score $\nabla_x\log p(x;\sigma)$ can be estimated by _Denoising Score Matching_ <d-cite key=song2019></d-cite>.Specifically, if $D_\theta(x;\sigma)$ is a neural network trained to minimize

$$
\mathbb{E}_{x_0\sim p_\mathrm{data}}\mathbb{E}_{\sigma \sim p_\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\mathbf{I})}\|D_\theta(x_0 +\sigma\epsilon;\sigma)-{x_0}\|_2^2,
$$

then

$$
\nabla_{x}\log p(x;\sigma)=\frac{D_\theta({x};\sigma)-x}{\sigma^2}
$$

**b. Replace $f(t),g(t)$ by $\sigma(t),s(t)$**

Substitute $f(t)=\frac{\dot{s}(t)} {s(t)}$ and $g(t)=s(t)\sqrt{2\dot{\sigma}(t)\sigma(t)}$ (equation 28 and 34 in <d-cite key=karras2022></d-cite>) into equation \eqref{eq:ode}, we get

$$
\mathrm{d}x=\left[ \frac{\dot{s}(t)}{s(t)}x-s(t)^2\dot{\sigma}(t)\sigma(t) \nabla_x\log p(\frac{x}{s(t)};\sigma(t)) \right]\mathrm{d}t
$$

By this equation, we can design new diffusion process by choosing $s(t)$ and $\sigma(t)$. For example, we can bring about an unusual process (but working) like in the below image.

<div class="row">
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/strange-ode.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
</div>
<div class="caption">
    image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a>
</div>

### 2. Improvements to deterministic sampling

#### Improvement 1 - Higher-order integrators

- $1^{st}$ order ODE solvers like Euler’s method lead to high error if step size is large
- High order Runge-Kutta methods require multiple evaluations of $D_\theta$ per step
- The authors claim that numerically solve the ODE using Heun’s 2nd order method provide an excellent tradeoff between error and NFE (neural function evaluations - how many times $D_\theta$ is evaluated)

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/heunsolver.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  image source: <a href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHHk4rFJw5yWFdsojXX_RTf-WYst939A0kjQ&s">math.libretexts.org</a>
</div>

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/alg1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

#### Improvement 2 - Discretization of time step

Appendix D.1 of <d-cite key=karras2022></d-cite> concludes that step size should decrease when $\sigma$ decrease. Intuitively, the closer we approach $x_0$ the slower we should move. Karras et al. <d-cite key=karras2022></d-cite> choose the noise level $\sigma$ at each discrete step $i$ by:

$$
\begin{aligned}
\sigma_{i < N} &= \left(\sigma_{\max}^{\frac1\rho}+\frac i{N-1}(\sigma_{\min}^{\frac1\rho}-\sigma_{\max}^{\frac1\rho})\right)^\rho \\
\sigma_N &= 0
\end{aligned}
$$

where $N$ is the number of denoising steps.
Given $\sigma_i$, the time step $t_i$ can be obtained via the inverse function of $\sigma(t)$, namely $t_i=\sigma^{-1}(\sigma_i)$

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/stepsize1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/stepsize2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  The EDM time step discretization has step sizes to be long at high noise levels and short at low noise levels as shown in the right figure (image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a>). <br> The left figure is the graph of the i-dependent noise level function given $\sigma_{\min}=80,\sigma_{\max}=0.002,\rho=7$
</div>

#### Improvement 3 - Make the ODE trajectory curvature to be low

Higher curvature trajectories leads to higher errors made by ODE numerical solver. Choice of $s(t),\sigma(t)$ can reduce or increase curvature of the flow lines as shown in the below figure.

<div class="row">
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/different-noise-schedules-flow-line-gif-1.gif" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
</div>
<div class="caption">
  image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a>
</div>

Karras et. al. <d-cite key=karras2022></d-cite> argues that the best choice which would significantly reduce the curvature is:

$$
\begin{aligned}
s(t)&=1 \\
\sigma(t)&=t
\end{aligned}
$$

which is also DDIM's choice.
Under this choice, the trajectory looks almost linear at both large and small $\sigma$, and substantial curvature lie in only a small region in between.

<div class="row">
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/ddim_trajectory.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
</div>
<div class="caption">
  the trajectory of DDIM/EDM ODE (image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a>)
</div>

{% details Why this specific choice of design make the trajectory largely linear? %}

When $\sigma(t)=t,s(t)=1$, the ODE is simplified as:

$$
\begin{aligned} \mathrm{d}x&=\left[ \frac{\dot{s}(t)}{s(t)}x-s(t)^2\dot{\sigma}(t)\sigma(t)\left( \frac{D_\theta({x};\sigma(t))-{x}}{\sigma(t)^2}\right)\right]\mathrm{d}t \\&= \left( \frac{x - D_\theta(x;t)}{t} \right)\mathrm{d}t \end{aligned}
$$

At any time step $t$, if we take a full single Euler step to $t=0$ (i.e. $\Delta t = -t$),

$$
\begin{aligned} x(t+(-t))&=x(t)+\left( \frac{x(t) - D_\theta(x;t)}{t} \right)(-t) \\ &=D_\theta(x;t) \end{aligned}
$$

That means the tangent of the trajectory always points towards the denoiser output. Because the denoiser output changes slowly with the noise level $\sigma$, the trajectory is largely linear

{% enddetails %}

### 3. Stochastic sampling

Deterministic sampling with ODE offers many benefits but sample quality is often worse then that of SDE. Karras et al. <d-cite key=karras2022></d-cite> argue that the reason is the SDE is sum of the probability flow ODE and a time-varying _Langevin diffusion_ SDE.
For example, as shown in Appendix B.5 of <d-cite key=karras2022></d-cite>, SDEs of Song et. al. <d-cite key=song2020></d-cite> can be expressed as:

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/sde_eqt.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

where $\beta(t)=\dot{\sigma}(t)/\sigma(t)$

Langevin diffusion actively correct for errors made in earlier sampling steps that results in better quality samples. An visual explanation for SDE's advantage can be found at <https://youtu.be/T0Qxzf0eaio?t=1946>

Karras et al. <d-cite key=karras2022></d-cite> propose a stochastic sampler which actually is the deterministic sampler being combined with explicitly adding and removing noise.

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/alg2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

The below figure illustrate the difference between the denoising steps of the stochastic and deterministic sampling algorithms

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/stochastic_sampler.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

In practice, values of $\{ S_\mathrm{churn},S_\mathrm{tmin},S_\mathrm{tmax},S_\mathrm{noise} \}$ have to be chosen carefully depending on the specific model, otherwise generated images would have issues such as loss of detail, over-saturation etc. (refer to the paper for details)

### 4. Preconditioning and training

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/edm-precondition.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

**Why we should not train $D_{\theta}$ directly?** It is advisable to keep input and output signal magnitudes ﬁxed to, e.g., unit variance, but $D_\theta$’s input and output are not. Recall that $D_\theta(x;\sigma)$ predicts clean signal $x_0$ from $x=x_0+\sigma \epsilon$ where $\epsilon \sim \mathcal{N}(0,\mathbf{I})$. Let

$$
D_\theta(x;\sigma)=x-\sigma F_\theta(.)
$$

will make the network predict the noise scaled to unit variance (similar to well-known $\epsilon$ prediction loss type). For input to have unit variance, we can use a $\sigma$-dependent normalization function.

**But $F_\theta$’s training target is always better than $D_\theta$’s?** Note that any errors made by $F_\theta$ are amplified by a factor of $\sigma$ → might harm the training at large $\sigma$

Karras et al. concluded that a $\sigma$-dependent mixing of training targets of $F_\theta$ and $D_\theta$ is needed. They re-write $D_\theta$ in the following form:

$$
D_\theta(x;\sigma)=c_\mathrm{skip}(\sigma)x+c_\mathrm{out}(\sigma)F_\theta(c_\mathrm{in}(\sigma)x;c_\mathrm{noise}(\sigma))
$$

| Function                   | Purpose                                                                                                                           |
| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| $c_\mathrm{out}(\sigma)$   | makes the output of $F_\theta$ be scaled to some variance                                                                         |
| $c_\mathrm{in}(\sigma)$    | works like a $\sigma$-dependent normalization factor that scales the input of $F_\theta$ to unit variance                         |
| $c_\mathrm{noise}(\sigma)$ | maps noise level $\sigma$ into a conditioning input for $F_\theta$. For example, in DDPM it maps noise levels to timestep indices |
| $c_\mathrm{skip}(\sigma)$  | $\sigma$-dependent skip connection that allows $F_\theta$ to estimate either $x_0$ or $\epsilon$, or something in between         |

:bulb: This formula is a generalization of $x_0$-prediction, $\epsilon$-prediction, $\mathrm{v}$-prediction target!

#### Determine suitable choices of preconditioning functions from first principles

The overall training loss:

$$
\mathbb{E}_{\sigma,y,n}\left[\lambda(\sigma)|| D(y+n;\sigma)-y ||^2_2\right], \\ \mathrm{where~} \sigma \sim p_{\mathrm{train}},y \sim p_{\mathrm{data}},n \sim \mathcal{N}(0,\sigma^2 \mathbf{I})
$$

can be expressed equivalently as:

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/loss.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

a. Training inputs of $F_\theta$ is required to have unit variance (Equation 114 ~ 117 of <d-cite key=karras2022></d-cite>):

$$
\mathrm{Var}_{y,n}[c_\mathrm{in}(\sigma)(y+n)]=1 \\
\to c_\mathrm{in}(\sigma)=\frac{1}{\sqrt{\sigma^2+\sigma^2_\mathrm{data}}}
$$

b. The effective training target $F_\mathrm{target}$ is required to have unit variance (Equation 118 ~ 123 of <d-cite key=karras2022></d-cite>):

$$
\mathrm{Var}_{y,n}[F_\mathrm{target}(y,n;\sigma)]=1 \\
\to c_\mathrm{out}(\sigma)^2=(1-c_\mathrm{skip}(\sigma))^2\sigma^2_\mathrm{data}+c_\mathrm{skip}(\sigma)^2\sigma^2
$$

c. We want errors of $F_\theta$ are amplified as little as possible, so we select $c_\mathrm{skip}(\sigma)$ to minimize $c_\mathrm{out}(\sigma)$ (Equation 124 ~ 131 of <d-cite key=karras2022></d-cite>):

$$
c_\mathrm{skip}(\sigma)=\mathrm{arg~min}_{c_\mathrm{skip}(\sigma)}c_\mathrm{out}(\sigma) \\
\to c_\mathrm{skip}(\sigma)=\frac{\sigma^2_\mathrm{data}} {(\sigma^2+\sigma^2_\mathrm{data})}
$$

substitute this into the above $c_\mathrm{out}$’s equation, we get:

$$
c_\mathrm{out}=\frac{\sigma.\sigma_\mathrm{data}}{\sqrt{\sigma^2+\sigma^2_\mathrm{data}}}
$$

d. The effective weight $\lambda(\sigma)c_\mathrm{out}(\sigma)^2$ is required to be uniform across noise levels $\sigma$:

$$
\lambda(\sigma)c_\mathrm{out}(\sigma)=1 \\
\to \lambda(\sigma)=(\sigma^2+\sigma^2_\mathrm{data})/(\sigma . \sigma_{\mathrm{data}})^2
$$

e. How to select $p_\mathrm{train}(\sigma)$, i.e., how to sample noise levels during training?

As shown by the Figure 5a in <d-cite key=karras2022></d-cite>, significant reduction between initial and final loss is only at intermediate noise levels. Therefore, the authors decide to target the training efforts to the relevant range as shown by the dashed red curve. Specifically, $p_\mathrm{train}(\sigma)$ is implicitly defined by a log-normal distribution:

$$
\mathrm{log}(\sigma) \sim \mathcal{N}(P_{\min},P^2_\mathrm{std}) \\
P_{\min}=-1.2, P_\mathrm{std}=1.2
$$

<div class="row">
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/sigma-sampling.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
</div>

f. Finally, $c_\mathrm{noise}(\sigma)=\frac{1}{4}\mathrm{ln}(\sigma)$ is chosen empirically

#### Augmentation regularization

Adopt an augmentation pipeline that consists of various geometric transformations (details in Appendix F.2 of <d-cite key=karras2022></d-cite>)

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/augmentation.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

To prevent the augmentations from leaking to the generated images, the authors make the augmentation parameters be a conditioning input to the neural network. During inference they are set to zero to ensure only non-augmented images are generated.

## Applications

The insights introduced by this outstanding paper bring about useful applications. I can think of two straightforward and direct applications w.r.t training and sampling.

One application is to retrain our denoiser in continuous space while adopting EDM formulation and the optimal configurations suggested in the paper.

Another appilcation is to boost sampling performance by applying the deterministic sampling in the paper. For example, based on Algorithm 1 and the recommended $\sigma(t)=t,s(t)=1$ we can implement an effective discrete Heun sampler that works with a pretrained DDPM model as follows.

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/edm/heun-ddpm.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class='caption'>
  This sampling resembles <a href="https://huggingface.co/docs/diffusers/en/api/schedulers/heun">HeunDiscreteScheduler</a> of Diffusers 
</div>
