<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Elucidating the design space of diffusion-based generative models (a.k.a EDM model) | Minh-Tuan Bui </title> <meta name="author" content="Minh-Tuan Bui"> <meta name="description" content="notes on the outstanding EDM paper"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/my-blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/my-blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/my-blog/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/my-blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/my-blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomasbuidev.github.io/my-blog/blog/2024/edm/"> <script src="/my-blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/my-blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/my-blog/assets/js/distillpub/template.v2.js"></script> <script src="/my-blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/my-blog/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Elucidating the design space of diffusion-based generative models (a.k.a EDM model)",
            "description": "notes on the outstanding EDM paper",
            "published": "October 10, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/my-blog/"> <span class="font-weight-bold">Minh-Tuan</span> Bui </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/my-blog/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/my-blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/my-blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/my-blog/projects/">projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/my-blog/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/my-blog/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/my-blog/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Elucidating the design space of diffusion-based generative models (a.k.a EDM model)</h1> <p>notes on the outstanding EDM paper</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#contributions">Contributions</a> </div> <div> <a href="#paper-contents">Paper contents</a> </div> <ul> <li> <a href="#1-expressing-diffusion-models-in-a-common-framework">1. Expressing diffusion models in a common framework</a> </li> <li> <a href="#2-improvements-to-deterministic-sampling">2. Improvements to deterministic sampling</a> </li> <li> <a href="#3-stochastic-sampling">3. Stochastic sampling</a> </li> <li> <a href="#4-preconditioning-and-training">4. Preconditioning and training</a> </li> </ul> <div> <a href="#applications">Applications</a> </div> </nav> </d-contents> <p>The EDM paper was introduced by Karras et al. <d-cite key="karras2022"></d-cite>. This excellent papers received Outstanding Papers Reward in NeurIPS 2022 <a href="https://arxiv.org/abs/2206.00364" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2206.00364</a></p> <h2 id="contributions">Contributions</h2> <p>As summarized in <d-footnote>https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/</d-footnote>, the main contributions of the paper are:</p> <ol> <li>A <strong>common framework</strong> for diffusion models</li> <li>Design choices related to <strong>sampling</strong> (generating images when you already have a trained denoiser)</li> <li>Design choices when <strong>training</strong> that denoiser</li> </ol> <hr> <h2 id="paper-contents">Paper contents</h2> <h3 id="1-expressing-diffusion-models-in-a-common-framework">1. Expressing diffusion models in a common framework</h3> <p>The Diffusion ODE can be expressed that:</p> \[\mathrm{d}x=[f(t)x-\frac{1}{2}g(t)^2\nabla_x\log p_t(x)]\mathrm{d}t\] <p>The issue is $f(t), g(t)$ and $p_t(x)$ not being quite useful when we like to design “working” variants of diffusion model because change in a component would lead to changes in the others in order to ensure the model converge to the data in limit. Therefore, the ODE need to be re-written to eliminate implicit dependencies between the components</p> <p><strong>a. Replace $p_t(x)$ by $p(x;\sigma)$</strong></p> <p>The perturbation kernels of Diffusion SDEs can be generalized as</p> \[\begin{aligned} x_t&amp;=s(t)\left( x_0 + \sigma(t)\epsilon\right) \textrm{ where } \epsilon \sim \mathcal{N}(0,\mathbf{I}) \end{aligned}\] <p>For example, SMLD SDE in <d-cite key="song2019"></d-cite> has</p> \[\begin{aligned} s(t)&amp;=1 \\ x(t)&amp;=x_0+\sigma(t) \epsilon \end{aligned}\] <p>And for DDPM SDE in <d-cite key="ho2020"></d-cite>,</p> \[\begin{aligned} s(t)&amp;=\frac{1}{\sqrt{\sigma(t)^2+1}} \\ x(t)&amp;=\frac{1}{\sqrt{\sigma(t)^2+1}}(x_0+\sigma(t) \epsilon) \end{aligned}\] <p>Equation 15 ~ 20 in <d-cite key="karras2022"></d-cite> proves that the marginal distribution $p_t(x)$ can be expressed as:</p> \[p_t(x)=\int p_{t}({x}\mid x_0)\mathrm{~}p_{\mathrm{data}}(x_0)\mathrm{~d}{x}=s(t)^{-d}p\left( \frac{x_t}{s(t)};\sigma(t)\right)\] <p>where $d$ is data dimension and $p(x;\sigma)$ is a distribution obtained by adding $\sigma$-standard deviation Gaussian noise to the original signal. Substitute $p_t(x)$ into the ODE equation, we get</p> <p>\begin{equation} \label{eq:ode} \mathrm{d}x = [f(t)x-\frac{1}{2}g(t)^2\nabla_x\log p_t(x)]\mathrm{d}t = \left[f(t){x}-\frac12g(t)^2 \nabla_x\log p(\frac{x}{s(t)};\sigma(t)) \right]\mathrm{~d}t \end{equation}</p> <p>(Refer to equation 21~24 in <d-cite key="karras2022"></d-cite> for the proof)</p> <p>$p(x;\sigma)$ is more favorable than $p_t(x)$ because it does not depend on $t$ and thus, the score $\nabla_x\log p(x;\sigma)$ does not depend on $\sigma(t),s(t)$ and how $t$ is discritized</p> <p>The score $\nabla_x\log p(x;\sigma)$ can be estimated by <em>Denoising Score Matching</em> <d-cite key="song2019"></d-cite>.Specifically, if $D_\theta(x;\sigma)$ is a neural network trained to minimize</p> \[\mathbb{E}_{x_0\sim p_\mathrm{data}}\mathbb{E}_{\sigma \sim p_\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\mathbf{I})}\|D_\theta(x_0 +\sigma\epsilon;\sigma)-{x_0}\|_2^2,\] <p>then</p> \[\nabla_{x}\log p(x;\sigma)=\frac{D_\theta({x};\sigma)-x}{\sigma^2}\] <p><strong>b. Replace $f(t),g(t)$ by $\sigma(t),s(t)$</strong></p> <p>Substitute $f(t)=\frac{\dot{s}(t)} {s(t)}$ and $g(t)=s(t)\sqrt{2\dot{\sigma}(t)\sigma(t)}$ (equation 28 and 34 in <d-cite key="karras2022"></d-cite>) into equation \eqref{eq:ode}, we get</p> \[\mathrm{d}x=\left[ \frac{\dot{s}(t)}{s(t)}x-s(t)^2\dot{\sigma}(t)\sigma(t) \nabla_x\log p(\frac{x}{s(t)};\sigma(t)) \right]\mathrm{d}t\] <p>By this equation, we can design new diffusion process by choosing $s(t)$ and $\sigma(t)$. For example, we can bring about an unusual process (but working) like in the below image.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/strange-ode-480.webp 480w,/my-blog/assets/img/edm/strange-ode-800.webp 800w,/my-blog/assets/img/edm/strange-ode-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/strange-ode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models" rel="external nofollow noopener" target="_blank">demystifying diffusion-based models</a> </div> <h3 id="2-improvements-to-deterministic-sampling">2. Improvements to deterministic sampling</h3> <h4 id="improvement-1---higher-order-integrators">Improvement 1 - Higher-order integrators</h4> <ul> <li>$1^{st}$ order ODE solvers like Euler’s method lead to high error if step size is large</li> <li>High order Runge-Kutta methods require multiple evaluations of $D_\theta$ per step</li> <li>The authors claim that numerically solve the ODE using Heun’s 2nd order method provide an excellent tradeoff between error and NFE (neural function evaluations - how many times $D_\theta$ is evaluated)</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/heunsolver-480.webp 480w,/my-blog/assets/img/edm/heunsolver-800.webp 800w,/my-blog/assets/img/edm/heunsolver-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/heunsolver.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> image source: <a href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHHk4rFJw5yWFdsojXX_RTf-WYst939A0kjQ&amp;s" rel="external nofollow noopener" target="_blank">math.libretexts.org</a> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/alg1-480.webp 480w,/my-blog/assets/img/edm/alg1-800.webp 800w,/my-blog/assets/img/edm/alg1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/alg1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="improvement-2---discretization-of-time-step">Improvement 2 - Discretization of time step</h4> <p>Appendix D.1 of <d-cite key="karras2022"></d-cite> concludes that step size should decrease when $\sigma$ decrease. Intuitively, the closer we approach $x_0$ the slower we should move. Karras et al. <d-cite key="karras2022"></d-cite> choose the noise level $\sigma$ at each discrete step $i$ by:</p> \[\begin{aligned} \sigma_{i &lt; N} &amp;= \left(\sigma_{\max}^{\frac1\rho}+\frac i{N-1}(\sigma_{\min}^{\frac1\rho}-\sigma_{\max}^{\frac1\rho})\right)^\rho \\ \sigma_N &amp;= 0 \end{aligned}\] <p>where $N$ is the number of denoising steps. Given $\sigma_i$, the time step $t_i$ can be obtained via the inverse function of $\sigma(t)$, namely $t_i=\sigma^{-1}(\sigma_i)$</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/stepsize1-480.webp 480w,/my-blog/assets/img/edm/stepsize1-800.webp 800w,/my-blog/assets/img/edm/stepsize1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/stepsize1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/stepsize2-480.webp 480w,/my-blog/assets/img/edm/stepsize2-800.webp 800w,/my-blog/assets/img/edm/stepsize2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/stepsize2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The EDM time step discretization has step sizes to be long at high noise levels and short at low noise levels as shown in the right figure (image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models" rel="external nofollow noopener" target="_blank">demystifying diffusion-based models</a>). <br> The left figure is the graph of the i-dependent noise level function given $\sigma_{\min}=80,\sigma_{\max}=0.002,\rho=7$ </div> <h4 id="improvement-3---make-the-ode-trajectory-curvature-to-be-low">Improvement 3 - Make the ODE trajectory curvature to be low</h4> <p>Higher curvature trajectories leads to higher errors made by ODE numerical solver. Choice of $s(t),\sigma(t)$ can reduce or increase curvature of the flow lines as shown in the below figure.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/different-noise-schedules-flow-line-gif-1-480.webp 480w,/my-blog/assets/img/edm/different-noise-schedules-flow-line-gif-1-800.webp 800w,/my-blog/assets/img/edm/different-noise-schedules-flow-line-gif-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/different-noise-schedules-flow-line-gif-1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models" rel="external nofollow noopener" target="_blank">demystifying diffusion-based models</a> </div> <p>Karras et. al. <d-cite key="karras2022"></d-cite> argues that the best choice which would significantly reduce the curvature is:</p> \[\begin{aligned} s(t)&amp;=1 \\ \sigma(t)&amp;=t \end{aligned}\] <p>which is also DDIM’s choice. Under this choice, the trajectory looks almost linear at both large and small $\sigma$, and substantial curvature lie in only a small region in between.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/ddim_trajectory-480.webp 480w,/my-blog/assets/img/edm/ddim_trajectory-800.webp 800w,/my-blog/assets/img/edm/ddim_trajectory-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/ddim_trajectory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> the trajectory of DDIM/EDM ODE (image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models" rel="external nofollow noopener" target="_blank">demystifying diffusion-based models</a>) </div> <details><summary>Why this specific choice of design make the trajectory largely linear?</summary> <p>When $\sigma(t)=t,s(t)=1$, the ODE is simplified as:</p> \[\begin{aligned} \mathrm{d}x&amp;=\left[ \frac{\dot{s}(t)}{s(t)}x-s(t)^2\dot{\sigma}(t)\sigma(t)\left( \frac{D_\theta({x};\sigma(t))-{x}}{\sigma(t)^2}\right)\right]\mathrm{d}t \\&amp;= \left( \frac{x - D_\theta(x;t)}{t} \right)\mathrm{d}t \end{aligned}\] <p>At any time step $t$, if we take a full single Euler step to $t=0$ (i.e. $\Delta t = -t$),</p> \[\begin{aligned} x(t+(-t))&amp;=x(t)+\left( \frac{x(t) - D_\theta(x;t)}{t} \right)(-t) \\ &amp;=D_\theta(x;t) \end{aligned}\] <p>That means the tangent of the trajectory always points towards the denoiser output. Because the denoiser output changes slowly with the noise level $\sigma$, the trajectory is largely linear</p> </details> <h3 id="3-stochastic-sampling">3. Stochastic sampling</h3> <p>Deterministic sampling with ODE offers many benefits but sample quality is often worse then that of SDE. Karras et al. <d-cite key="karras2022"></d-cite> argue that the reason is the SDE is sum of the probability flow ODE and a time-varying <em>Langevin diffusion</em> SDE. For example, as shown in Appendix B.5 of <d-cite key="karras2022"></d-cite>, SDEs of Song et. al. <d-cite key="song2020"></d-cite> can be expressed as:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/sde_eqt-480.webp 480w,/my-blog/assets/img/edm/sde_eqt-800.webp 800w,/my-blog/assets/img/edm/sde_eqt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/sde_eqt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>where $\beta(t)=\dot{\sigma}(t)/\sigma(t)$</p> <p>Langevin diffusion actively correct for errors made in earlier sampling steps that results in better quality samples. An visual explanation for SDE’s advantage can be found at <a href="https://youtu.be/T0Qxzf0eaio?t=1946" rel="external nofollow noopener" target="_blank">https://youtu.be/T0Qxzf0eaio?t=1946</a></p> <p>Karras et al. <d-cite key="karras2022"></d-cite> propose a stochastic sampler which actually is the deterministic sampler being combined with explicitly adding and removing noise.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/alg2-480.webp 480w,/my-blog/assets/img/edm/alg2-800.webp 800w,/my-blog/assets/img/edm/alg2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/alg2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The below figure illustrate the difference between the denoising steps of the stochastic and deterministic sampling algorithms</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/stochastic_sampler-480.webp 480w,/my-blog/assets/img/edm/stochastic_sampler-800.webp 800w,/my-blog/assets/img/edm/stochastic_sampler-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/stochastic_sampler.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In practice, values of ${ S_\mathrm{churn},S_\mathrm{tmin},S_\mathrm{tmax},S_\mathrm{noise} }$ have to be chosen carefully depending on the specific model, otherwise generated images would have issues such as loss of detail, over-saturation etc. (refer to the paper for details)</p> <h3 id="4-preconditioning-and-training">4. Preconditioning and training</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/edm-precondition-480.webp 480w,/my-blog/assets/img/edm/edm-precondition-800.webp 800w,/my-blog/assets/img/edm/edm-precondition-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/edm-precondition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Why we should not train $D_{\theta}$ directly?</strong> It is advisable to keep input and output signal magnitudes ﬁxed to, e.g., unit variance, but $D_\theta$’s input and output are not. Recall that $D_\theta(x;\sigma)$ predicts clean signal $x_0$ from $x=x_0+\sigma \epsilon$ where $\epsilon \sim \mathcal{N}(0,\mathbf{I})$. Let</p> \[D_\theta(x;\sigma)=x-\sigma F_\theta(.)\] <p>will make the network predict the noise scaled to unit variance (similar to well-known $\epsilon$ prediction loss type). For input to have unit variance, we can use a $\sigma$-dependent normalization function.</p> <p><strong>But $F_\theta$’s training target is always better than $D_\theta$’s?</strong> Note that any errors made by $F_\theta$ are amplified by a factor of $\sigma$ → might harm the training at large $\sigma$</p> <p>Karras et al. concluded that a $\sigma$-dependent mixing of training targets of $F_\theta$ and $D_\theta$ is needed. They re-write $D_\theta$ in the following form:</p> \[D_\theta(x;\sigma)=c_\mathrm{skip}(\sigma)x+c_\mathrm{out}(\sigma)F_\theta(c_\mathrm{in}(\sigma)x;c_\mathrm{noise}(\sigma))\] <table> <thead> <tr> <th>Function</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>$c_\mathrm{out}(\sigma)$</td> <td>makes the output of $F_\theta$ be scaled to some variance</td> </tr> <tr> <td>$c_\mathrm{in}(\sigma)$</td> <td>works like a $\sigma$-dependent normalization factor that scales the input of $F_\theta$ to unit variance</td> </tr> <tr> <td>$c_\mathrm{noise}(\sigma)$</td> <td>maps noise level $\sigma$ into a conditioning input for $F_\theta$. For example, in DDPM it maps noise levels to timestep indices</td> </tr> <tr> <td>$c_\mathrm{skip}(\sigma)$</td> <td>$\sigma$-dependent skip connection that allows $F_\theta$ to estimate either $x_0$ or $\epsilon$, or something in between</td> </tr> </tbody> </table> <p><img class="emoji" title=":bulb:" alt=":bulb:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png" height="20" width="20"> This formula is a generalization of $x_0$-prediction, $\epsilon$-prediction, $\mathrm{v}$-prediction target!</p> <h4 id="determine-suitable-choices-of-preconditioning-functions-from-first-principles">Determine suitable choices of preconditioning functions from first principles</h4> <p>The overall training loss:</p> \[\mathbb{E}_{\sigma,y,n}\left[\lambda(\sigma)|| D(y+n;\sigma)-y ||^2_2\right], \\ \mathrm{where~} \sigma \sim p_{\mathrm{train}},y \sim p_{\mathrm{data}},n \sim \mathcal{N}(0,\sigma^2 \mathbf{I})\] <p>can be expressed equivalently as:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/loss-480.webp 480w,/my-blog/assets/img/edm/loss-800.webp 800w,/my-blog/assets/img/edm/loss-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>a. Training inputs of $F_\theta$ is required to have unit variance (Equation 114 ~ 117 of <d-cite key="karras2022"></d-cite>):</p> \[\mathrm{Var}_{y,n}[c_\mathrm{in}(\sigma)(y+n)]=1 \\ \to c_\mathrm{in}(\sigma)=\frac{1}{\sqrt{\sigma^2+\sigma^2_\mathrm{data}}}\] <p>b. The effective training target $F_\mathrm{target}$ is required to have unit variance (Equation 118 ~ 123 of <d-cite key="karras2022"></d-cite>):</p> \[\mathrm{Var}_{y,n}[F_\mathrm{target}(y,n;\sigma)]=1 \\ \to c_\mathrm{out}(\sigma)^2=(1-c_\mathrm{skip}(\sigma))^2\sigma^2_\mathrm{data}+c_\mathrm{skip}(\sigma)^2\sigma^2\] <p>c. We want errors of $F_\theta$ are amplified as little as possible, so we select $c_\mathrm{skip}(\sigma)$ to minimize $c_\mathrm{out}(\sigma)$ (Equation 124 ~ 131 of <d-cite key="karras2022"></d-cite>):</p> \[c_\mathrm{skip}(\sigma)=\mathrm{arg~min}_{c_\mathrm{skip}(\sigma)}c_\mathrm{out}(\sigma) \\ \to c_\mathrm{skip}(\sigma)=\frac{\sigma^2_\mathrm{data}} {(\sigma^2+\sigma^2_\mathrm{data})}\] <p>substitute this into the above $c_\mathrm{out}$’s equation, we get:</p> \[c_\mathrm{out}=\frac{\sigma.\sigma_\mathrm{data}}{\sqrt{\sigma^2+\sigma^2_\mathrm{data}}}\] <p>d. The effective weight $\lambda(\sigma)c_\mathrm{out}(\sigma)^2$ is required to be uniform across noise levels $\sigma$:</p> \[\lambda(\sigma)c_\mathrm{out}(\sigma)=1 \\ \to \lambda(\sigma)=(\sigma^2+\sigma^2_\mathrm{data})/(\sigma . \sigma_{\mathrm{data}})^2\] <p>e. How to select $p_\mathrm{train}(\sigma)$, i.e., how to sample noise levels during training?</p> <p>As shown by the Figure 5a in <d-cite key="karras2022"></d-cite>, significant reduction between initial and final loss is only at intermediate noise levels. Therefore, the authors decide to target the training efforts to the relevant range as shown by the dashed red curve. Specifically, $p_\mathrm{train}(\sigma)$ is implicitly defined by a log-normal distribution:</p> \[\mathrm{log}(\sigma) \sim \mathcal{N}(P_{\min},P^2_\mathrm{std}) \\ P_{\min}=-1.2, P_\mathrm{std}=1.2\] <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/sigma-sampling-480.webp 480w,/my-blog/assets/img/edm/sigma-sampling-800.webp 800w,/my-blog/assets/img/edm/sigma-sampling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/sigma-sampling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>f. Finally, $c_\mathrm{noise}(\sigma)=\frac{1}{4}\mathrm{ln}(\sigma)$ is chosen empirically</p> <h4 id="augmentation-regularization">Augmentation regularization</h4> <p>Adopt an augmentation pipeline that consists of various geometric transformations (details in Appendix F.2 of <d-cite key="karras2022"></d-cite>)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/augmentation-480.webp 480w,/my-blog/assets/img/edm/augmentation-800.webp 800w,/my-blog/assets/img/edm/augmentation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/augmentation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To prevent the augmentations from leaking to the generated images, the authors make the augmentation parameters be a conditioning input to the neural network. During inference they are set to zero to ensure only non-augmented images are generated.</p> <hr> <h2 id="applications">Applications</h2> <p>The insights introduced by this outstanding paper bring about useful applications. I can think of two straightforward and direct applications w.r.t training and sampling.</p> <p>One application is to retrain our denoiser in continuous space while adopting EDM formulation and the optimal configurations suggested in the paper.</p> <p>Another appilcation is to boost sampling performance by applying the deterministic sampling in the paper. For example, based on Algorithm 1 and the recommended $\sigma(t)=t,s(t)=1$ we can implement an effective discrete Heun sampler that works with a pretrained DDPM model as follows.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/my-blog/assets/img/edm/heun-ddpm-480.webp 480w,/my-blog/assets/img/edm/heun-ddpm-800.webp 800w,/my-blog/assets/img/edm/heun-ddpm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/my-blog/assets/img/edm/heun-ddpm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> This sampling resembles <a href="https://huggingface.co/docs/diffusers/en/api/schedulers/heun" rel="external nofollow noopener" target="_blank">HeunDiscreteScheduler</a> of Diffusers </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/my-blog/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Minh-Tuan Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/my-blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/my-blog/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/my-blog/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/my-blog/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/my-blog/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/my-blog/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/my-blog/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/my-blog/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/my-blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/my-blog/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/my-blog/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/my-blog/assets/js/search-data.js"></script> <script src="/my-blog/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>